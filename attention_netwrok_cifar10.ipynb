{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention netwrok cifar10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/e5OtYzCIpIZBELIGDlKB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hasnat65/thesis42/blob/master/attention_netwrok_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNl5UcFJAEfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import UpSampling2D\n",
        "from keras.layers import Activation\n",
        "from keras.layers import MaxPool2D\n",
        "from keras.layers import Add\n",
        "from keras.layers import Multiply\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPool2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import AveragePooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Activation\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmmaj9d8-PBt",
        "colab_type": "text"
      },
      "source": [
        "residual **and** attention blocks\n",
        "**bold text** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dtma7ga_G9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "def residual_block(input, input_channels=None, output_channels=None, kernel_size=(3, 3), stride=1):\n",
        "    \"\"\"\n",
        "    full pre-activation residual block\n",
        "    https://arxiv.org/pdf/1603.05027.pdf\n",
        "    \"\"\"\n",
        "    if output_channels is None:\n",
        "        output_channels = input.get_shape()[-1].value\n",
        "    if input_channels is None:\n",
        "        input_channels = output_channels // 4\n",
        "\n",
        "    strides = (stride, stride)\n",
        "\n",
        "    x = BatchNormalization()(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(input_channels, (1, 1))(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(input_channels, kernel_size, padding='same', strides=stride)(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(output_channels, (1, 1), padding='same')(x)\n",
        "\n",
        "    if input_channels != output_channels or stride != 1:\n",
        "        input = Conv2D(output_channels, (1, 1), padding='same', strides=strides)(input)\n",
        "\n",
        "    x = Add()([x, input])\n",
        "    return x\n",
        "\n",
        "\n",
        "def attention_block(input, input_channels=None, output_channels=None, encoder_depth=1):\n",
        "    \"\"\"\n",
        "    attention block\n",
        "    https://arxiv.org/abs/1704.06904\n",
        "    \"\"\"\n",
        "\n",
        "    p = 1\n",
        "    t = 2\n",
        "    r = 1\n",
        "\n",
        "    if input_channels is None:\n",
        "        input_channels = input.get_shape()[-1].value\n",
        "    if output_channels is None:\n",
        "        output_channels = input_channels\n",
        "\n",
        "    # First Residual Block\n",
        "    for i in range(p):\n",
        "        input = residual_block(input)\n",
        "\n",
        "    # Trunc Branch\n",
        "    output_trunk = input\n",
        "    for i in range(t):\n",
        "        output_trunk = residual_block(output_trunk)\n",
        "\n",
        "    # Soft Mask Branch\n",
        "\n",
        "    ## encoder\n",
        "    ### first down sampling\n",
        "    output_soft_mask = MaxPool2D(padding='same')(input)  # 32x32\n",
        "    for i in range(r):\n",
        "        output_soft_mask = residual_block(output_soft_mask)\n",
        "\n",
        "    skip_connections = []\n",
        "    for i in range(encoder_depth - 1):\n",
        "\n",
        "        ## skip connections\n",
        "        output_skip_connection = residual_block(output_soft_mask)\n",
        "        skip_connections.append(output_skip_connection)\n",
        "        # print ('skip shape:', output_skip_connection.get_shape())\n",
        "\n",
        "        ## down sampling\n",
        "        output_soft_mask = MaxPool2D(padding='same')(output_soft_mask)\n",
        "        for _ in range(r):\n",
        "            output_soft_mask = residual_block(output_soft_mask)\n",
        "\n",
        "            ## decoder\n",
        "    skip_connections = list(reversed(skip_connections))\n",
        "    for i in range(encoder_depth - 1):\n",
        "        ## upsampling\n",
        "        for _ in range(r):\n",
        "            output_soft_mask = residual_block(output_soft_mask)\n",
        "        output_soft_mask = UpSampling2D()(output_soft_mask)\n",
        "        ## skip connections\n",
        "        output_soft_mask = Add()([output_soft_mask, skip_connections[i]])\n",
        "\n",
        "    ### last upsampling\n",
        "    for i in range(r):\n",
        "        output_soft_mask = residual_block(output_soft_mask)\n",
        "    output_soft_mask = UpSampling2D()(output_soft_mask)\n",
        "\n",
        "    ## Output\n",
        "    output_soft_mask = Conv2D(input_channels, (1, 1))(output_soft_mask)\n",
        "    output_soft_mask = Conv2D(input_channels, (1, 1))(output_soft_mask)\n",
        "    output_soft_mask = Activation('sigmoid')(output_soft_mask)\n",
        "\n",
        "    # Attention: (1 + output_soft_mask) * output_trunk\n",
        "    output = Lambda(lambda x: x + 1)(output_soft_mask)\n",
        "    output = Multiply()([output, output_trunk])  #\n",
        "\n",
        "    # Last Residual Block\n",
        "    for i in range(p):\n",
        "        output = residual_block(output)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7_jhBsK_M9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "def AttentionResNetCifar10(shape=(32, 32, 3), n_channels=32, n_classes=10):\n",
        "    \"\"\"\n",
        "    Attention-56 ResNet for Cifar10 Dataset\n",
        "    https://arxiv.org/abs/1704.06904\n",
        "    \"\"\"\n",
        "    input_ = Input(shape=shape)\n",
        "    x = Conv2D(n_channels, (5, 5), padding='same')(input_)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x)  # 16x16\n",
        "\n",
        "    x = residual_block(x, input_channels=32, output_channels=128)\n",
        "    x = attention_block(x, encoder_depth=2)\n",
        "\n",
        "    x = residual_block(x, input_channels=128, output_channels=256, stride=2)  # 8x8\n",
        "    x = attention_block(x, encoder_depth=1)\n",
        "\n",
        "    x = residual_block(x, input_channels=256, output_channels=512, stride=2)  # 4x4\n",
        "    x = attention_block(x, encoder_depth=1)\n",
        "\n",
        "    x = residual_block(x, input_channels=512, output_channels=1024)\n",
        "    x = residual_block(x, input_channels=1024, output_channels=1024)\n",
        "    x = residual_block(x, input_channels=1024, output_channels=1024)\n",
        "\n",
        "    x = AveragePooling2D(pool_size=(4, 4), strides=(1, 1))(x)  # 1x1\n",
        "    x = Flatten()(x)\n",
        "    output = Dense(n_classes, activation='softmax')(x)\n",
        "    model = Model(input_, output)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqbSQ1nm_RHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = AttentionResNetCifar10(n_classes=10)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "359R3GbP8ZgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        " \n",
        "# load data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2u91JkBCuHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import keras\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKnjbb1M86AI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define generators for training and validation data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True)\n",
        "\n",
        "# compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "train_datagen.fit(x_train)\n",
        "val_datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leaHAhg-9Kp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare usefull callbacks\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=7, min_lr=10e-7, epsilon=0.01, verbose=1)\n",
        "early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=15, verbose=1)\n",
        "callbacks= [lr_reducer, early_stopper]\n",
        " \n",
        "# define loss, metrics, optimizer\n",
        "model.compile(keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        " \n",
        "# fits the model on batches with real-time data augmentation\n",
        "batch_size = 32\n",
        "\n",
        "model.fit_generator(train_datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(x_train)//batch_size, epochs=50,\n",
        "                    validation_data=val_datagen.flow(x_test, y_test, batch_size=batch_size), \n",
        "                    validation_steps=len(x_test)//batch_size,\n",
        "                    callbacks=callbacks, initial_epoch=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y622gnyfiY2x",
        "colab_type": "text"
      },
      "source": [
        "**Validation Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76vbMi0FhJiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb24e152-dcce-40fe-d335-f56ac637a5b7"
      },
      "source": [
        "model.evaluate_generator(val_datagen.flow(x_test,y_test), steps= len(x_test)/32, use_multiprocessing = True )"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3270844711691141, 0.8999]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}